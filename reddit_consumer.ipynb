{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkStreaming Hackathon\n",
    "### Course: Real-time Data Analysis\n",
    "### Authors: Ruben Tak, Nils Jennissen, David Landeo\n",
    "This task involves setting up a data streaming pipeline to extract and process posts and comments from Reddit. The data will be structured and sent through a socket, then received and processed by another process. References to users, posts, and external sites will be extracted and counted, and the top 10 important words will be identified using TF-IDF. Optional features include sentiment analysis, additional metrics, saving results to a database, creating a Jupyter Notebook dashboard, and visualizing the results on a web page. The deliverables include Python code, instructions, output data files, and optional Docker setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-06-21T19:37:26.726664Z",
     "start_time": "2023-06-21T19:37:24.738614Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/21 21:37:26 WARN Utils: Your hostname, Nilss-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.108 instead (on interface en0)\n",
      "23/06/21 21:37:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Exception in thread \"main\" java.lang.ExceptionInInitializerError\n",
      "\tat org.apache.spark.unsafe.array.ByteArrayMethods.<clinit>(ByteArrayMethods.java:54)\n",
      "\tat org.apache.spark.internal.config.package$.<init>(package.scala:1006)\n",
      "\tat org.apache.spark.internal.config.package$.<clinit>(package.scala)\n",
      "\tat org.apache.spark.deploy.SparkSubmitArguments.$anonfun$loadEnvironmentArguments$3(SparkSubmitArguments.scala:157)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.deploy.SparkSubmitArguments.loadEnvironmentArguments(SparkSubmitArguments.scala:157)\n",
      "\tat org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:115)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:990)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:990)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:85)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make private java.nio.DirectByteBuffer(long,int) accessible: module java.base does not \"opens java.nio\" to unnamed module @15c89abd\n",
      "\tat java.base/java.lang.reflect.AccessibleObject.throwInaccessibleObjectException(AccessibleObject.java:387)\n",
      "\tat java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:363)\n",
      "\tat java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:311)\n",
      "\tat java.base/java.lang.reflect.Constructor.checkCanSetAccessible(Constructor.java:192)\n",
      "\tat java.base/java.lang.reflect.Constructor.setAccessible(Constructor.java:185)\n",
      "\tat org.apache.spark.unsafe.Platform.<clinit>(Platform.java:56)\n",
      "\t... 13 more\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 163\u001B[0m\n\u001B[1;32m    153\u001B[0m schema \u001B[38;5;241m=\u001B[39m StructType([\n\u001B[1;32m    154\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[1;32m    155\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    159\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcreated_utc\u001B[39m\u001B[38;5;124m\"\u001B[39m, FloatType(), \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    160\u001B[0m ])\n\u001B[1;32m    162\u001B[0m \u001B[38;5;66;03m# Create a SparkSession\u001B[39;00m\n\u001B[0;32m--> 163\u001B[0m spark \u001B[38;5;241m=\u001B[39m \u001B[43mSparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappName\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mreddit\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;66;03m# Read the data from the socket as a streaming DataFrame\u001B[39;00m\n\u001B[1;32m    166\u001B[0m raw_data \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mreadStream\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msocket\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhost\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlocalhost\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mport\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m9999\u001B[39m)\u001B[38;5;241m.\u001B[39mload()\n",
      "File \u001B[0;32m~/miniforge3/envs/spark-streaming-hackathon/lib/python3.10/site-packages/pyspark/sql/session.py:186\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    184\u001B[0m         sparkConf\u001B[38;5;241m.\u001B[39mset(key, value)\n\u001B[1;32m    185\u001B[0m     \u001B[38;5;66;03m# This SparkContext may be an existing one.\u001B[39;00m\n\u001B[0;32m--> 186\u001B[0m     sc \u001B[38;5;241m=\u001B[39m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msparkConf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[39;00m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;66;03m# by all sessions.\u001B[39;00m\n\u001B[1;32m    189\u001B[0m session \u001B[38;5;241m=\u001B[39m SparkSession(sc)\n",
      "File \u001B[0;32m~/miniforge3/envs/spark-streaming-hackathon/lib/python3.10/site-packages/pyspark/context.py:378\u001B[0m, in \u001B[0;36mSparkContext.getOrCreate\u001B[0;34m(cls, conf)\u001B[0m\n\u001B[1;32m    376\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    377\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 378\u001B[0m         \u001B[43mSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mSparkConf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    379\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\n",
      "File \u001B[0;32m~/miniforge3/envs/spark-streaming-hackathon/lib/python3.10/site-packages/pyspark/context.py:133\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    129\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    130\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    131\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 133\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001B[1;32m    136\u001B[0m                   conf, jsc, profiler_cls)\n",
      "File \u001B[0;32m~/miniforge3/envs/spark-streaming-hackathon/lib/python3.10/site-packages/pyspark/context.py:327\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_gateway:\n\u001B[0;32m--> 327\u001B[0m         SparkContext\u001B[38;5;241m.\u001B[39m_gateway \u001B[38;5;241m=\u001B[39m gateway \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mlaunch_gateway\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    328\u001B[0m         SparkContext\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_gateway\u001B[38;5;241m.\u001B[39mjvm\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m instance:\n",
      "File \u001B[0;32m~/miniforge3/envs/spark-streaming-hackathon/lib/python3.10/site-packages/pyspark/java_gateway.py:105\u001B[0m, in \u001B[0;36mlaunch_gateway\u001B[0;34m(conf, popen_kwargs)\u001B[0m\n\u001B[1;32m    102\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.1\u001B[39m)\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(conn_info_file):\n\u001B[0;32m--> 105\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJava gateway process exited before sending its port number\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(conn_info_file, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m info:\n\u001B[1;32m    108\u001B[0m     gateway_port \u001B[38;5;241m=\u001B[39m read_int(info)\n",
      "\u001B[0;31mException\u001B[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql.functions import from_json, regexp_extract, split, window, count, min, max, avg, udf\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from textblob import TextBlob\n",
    "from pyspark.sql.functions import explode, to_date\n",
    "from pyspark.sql.types import TimestampType\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from typing import Iterator\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_comment(df):\n",
    "    return df.withColumn(\"words\", split(df[\"comment\"], \" \"))\n",
    "\n",
    "def calculate_tfidf(df):\n",
    "    if df.rdd.isEmpty():\n",
    "        return None, None, None\n",
    "\n",
    "    vectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"raw_features\")\n",
    "    vectorizer_model = vectorizer.fit(df)\n",
    "    count_vectorized = vectorizer_model.transform(df)\n",
    "\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "    idf_model = idf.fit(count_vectorized)\n",
    "    tfidf = idf_model.transform(count_vectorized)\n",
    "\n",
    "    return tfidf, vectorizer_model, idf_model\n",
    "\n",
    "def extract_references(df):\n",
    "    return df.select(\"comment\", \"prev_comment\", \"post\", \"created_utc\",\n",
    "                     regexp_extract(\"comment\", '/u/([^\\\\s/]+)', 1).alias('user_reference'),\n",
    "                     regexp_extract(\"comment\", '/r/([^\\\\s/]+)', 1).alias('post_reference'),\n",
    "                     regexp_extract(\"comment\", 'http[s]?://([^\\\\s/]+)', 1).alias('site_reference'))\n",
    "\n",
    "def calculate_time_range(df):\n",
    "    # Filter rows with non-null values in the created_utc column\n",
    "    df = df.filter(df[\"created_utc\"].isNotNull())\n",
    "\n",
    "    if df.rdd.isEmpty():\n",
    "        return None, None\n",
    "\n",
    "    time_range = df.agg(min(\"created_utc\").alias(\"min_date\"), max(\"created_utc\").alias(\"max_date\")).collect()[0]\n",
    "    return time_range[\"min_date\"], time_range[\"max_date\"]\n",
    "\n",
    "\n",
    "def calculate_sentiment(df):\n",
    "    if df.rdd.isEmpty():\n",
    "        return df\n",
    "\n",
    "    @pandas_udf(FloatType())\n",
    "    def sentiment_score(series: pd.Series) -> pd.Series:\n",
    "        return series.apply(lambda text: TextBlob(text).sentiment.polarity)\n",
    "\n",
    "    df = df.withColumn(\"sentiment\", sentiment_score(df[\"comment\"]))\n",
    "    return df\n",
    "\n",
    "def most_common_words(df, n=10):\n",
    "    words_df = df.select(explode(split(df[\"post\"], \" \")).alias(\"word\"))\n",
    "    word_counts = words_df.groupBy(\"word\").agg(count(\"*\").alias(\"count\")).orderBy(\"count\", ascending=False)\n",
    "    return word_counts.limit(n)\n",
    "\n",
    "def process_batch(df, epoch_id):\n",
    "    if df.rdd.isEmpty():\n",
    "        return\n",
    "\n",
    "    # Preprocess comments\n",
    "    preprocessed_comments = preprocess_comment(df)\n",
    "\n",
    "    # Print schema and sample data for debugging\n",
    "    preprocessed_comments.printSchema()\n",
    "    preprocessed_comments.show(5)\n",
    "\n",
    "    # Calculate TF-IDF\n",
    "    tfidf, vectorizer_model, idf_model = calculate_tfidf(preprocessed_comments)\n",
    "\n",
    "    if vectorizer_model and idf_model:\n",
    "        # Get top 10 important words\n",
    "        vocab = vectorizer_model.vocabulary\n",
    "        top_10_words = idf_model.idf.toArray().argsort()[-10:]\n",
    "        top_10_words = [vocab[idx] for idx in top_10_words]\n",
    "        if top_10_words:\n",
    "            print(\"Top 10 important words:\")\n",
    "            print(top_10_words)\n",
    "\n",
    "    # Extract references from the data\n",
    "    references_df = extract_references(preprocessed_comments)\n",
    "\n",
    "    # Save the raw data to a temporary table in Spark\n",
    "    preprocessed_comments.createOrReplaceTempView(\"raw\")\n",
    "\n",
    "    # Save the raw data to disk\n",
    "    preprocessed_comments.write.json(\"output/raw\", mode=\"append\")\n",
    "\n",
    "    # Calculate occurrences of references\n",
    "    user_ref_counts = references_df.groupBy(window(\"created_utc\", \"60 seconds\", \"5 seconds\"), \"user_reference\").agg(count(\"*\").alias(\"count\")).orderBy(\"window\", \"count\", ascending=False)\n",
    "    post_ref_counts = references_df.groupBy(window(\"created_utc\", \"60 seconds\", \"5 seconds\"), \"post_reference\").agg(count(\"*\").alias(\"count\")).orderBy(\"window\", \"count\", ascending=False)\n",
    "    site_ref_counts = references_df.groupBy(window(\"created_utc\", \"60 seconds\", \"5 seconds\"), \"site_reference\").agg(count(\"*\").alias(\"count\")).orderBy(\"window\", \"count\", ascending=False)\n",
    "\n",
    "    # Print the occurrences if not empty\n",
    "    if not user_ref_counts.rdd.isEmpty():\n",
    "        print(\"User references:\")\n",
    "        user_ref_counts.show()\n",
    "    if not post_ref_counts.rdd.isEmpty():\n",
    "        print(\"Post references:\")\n",
    "        post_ref_counts.show()\n",
    "    if not site_ref_counts.rdd.isEmpty():\n",
    "        print(\"Site references:\")\n",
    "        site_ref_counts.show()\n",
    "\n",
    "    # Calculate time range\n",
    "    min_date, max_date = calculate_time_range(preprocessed_comments)\n",
    "    if min_date and max_date:\n",
    "        print(f\"Time range of the data: {min_date} - {max_date}\")\n",
    "\n",
    "    # Calculate sentiment\n",
    "    sentiment_df = calculate_sentiment(preprocessed_comments)\n",
    "    if not sentiment_df.rdd.isEmpty():\n",
    "        avg_sentiment = sentiment_df.agg(avg(\"sentiment\").alias(\"average_sentiment\")).collect()[0][\"average_sentiment\"]\n",
    "        print(f\"Average sentiment: {avg_sentiment}\")\n",
    "    else:\n",
    "        print(\"No sentiment data available.\")\n",
    "\n",
    "    # Calculate most common words\n",
    "    common_words_df = most_common_words(preprocessed_comments)\n",
    "    print(\"Most common words in post titles:\")\n",
    "    common_words_df.show()\n",
    "\n",
    "def calculate_sentiment(df):\n",
    "    if df.rdd.isEmpty():\n",
    "        return df\n",
    "\n",
    "    @pandas_udf(FloatType(), PandasUDFType.SCALAR)\n",
    "    def sentiment_score(series: pd.Series) -> pd.Series:\n",
    "        return series.apply(lambda text: TextBlob(text).sentiment.polarity)\n",
    "\n",
    "    df = df.withColumn(\"sentiment\", sentiment_score(df[\"comment\"]))\n",
    "    return df\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"reddit\").getOrCreate()\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"comment\", StringType(), True),\n",
    "    StructField(\"prev_comment\", StringType(), True),\n",
    "    StructField(\"post\", StringType(), True),\n",
    "    StructField(\"created_utc\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Read the data from the socket as a streaming DataFrame\n",
    "raw_data = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\", 9999).load()\n",
    "\n",
    "# Parse the JSON data and apply the schema\n",
    "parsed_data = raw_data.select(from_json(raw_data.value, schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Convert the created_utc field to TimestampType\n",
    "parsed_data = parsed_data.withColumn(\"created_utc\", from_unixtime(parsed_data[\"created_utc\"]).cast(TimestampType()))\n",
    "\n",
    "# Process each batch of data\n",
    "query = parsed_data.writeStream.foreachBatch(process_batch).start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "1db35aecd77fcc020a8642668a5b7619c380276ef0ce04f324d75e02f2d7512b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
