{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkStreaming Hackathon\n",
    "### Course: Real-time Data Analysis\n",
    "### Authors: Ruben Tak, Nils Jennissen, David Landeo\n",
    "This task involves setting up a data streaming pipeline to extract and process posts and comments from Reddit. The data will be structured and sent through a socket, then received and processed by another process. References to users, posts, and external sites will be extracted and counted, and the top 10 important words will be identified using TF-IDF. Optional features include sentiment analysis, additional metrics, saving results to a database, creating a Jupyter Notebook dashboard, and visualizing the results on a web page. The deliverables include Python code, instructions, output data files, and optional Docker setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-06-21T13:20:50.611075Z",
     "start_time": "2023-06-21T13:20:48.562624Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/21 15:20:49 WARN Utils: Your hostname, Nilss-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.108 instead (on interface en0)\n",
      "23/06/21 15:20:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/21 15:20:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/Users/nilsjennissen/miniforge3/envs/spark-streaming-hackathon/lib/python3.10/site-packages/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "# Create a SparkSession and StreamingContext\n",
    "spark_conf = SparkConf().setAppName(\"reddit\")\n",
    "ss1 = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "ssc = StreamingContext(ss1.sparkContext, 40)\n",
    "\n",
    "# Create a DStream\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "comments = lines.map(lambda json_data: json.loads(json_data))\n",
    "comments.pprint()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"comment\", StringType(), True),\n",
    "    StructField(\"parent_comment\", StringType(), True),\n",
    "    StructField(\"post\", StringType(), True),\n",
    "    StructField(\"created_utc\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "])\n",
    "\n",
    "base_path = \"./data/raw/reddit_v5\"\n",
    "\n",
    "def process_rdd(time, rdd):\n",
    "    if not rdd.isEmpty():\n",
    "        df = ss1.createDataFrame(rdd, schema)\n",
    "        df.createOrReplaceTempView(\"raw\")\n",
    "        df.persist()\n",
    "        output_path = f\"{base_path}/{time.strftime('%Y%m%d%H%M%S')}\"\n",
    "        df.write.json(output_path)\n",
    "        df.write.csv(output_path + \".csv\", header=True)\n",
    "\n",
    "        # Get references to users, posts, and external sites\n",
    "        references_df = ss1.sql(\"\"\"\n",
    "            SELECT \n",
    "                comment,\n",
    "                prev_comment,\n",
    "                post,\n",
    "                author,\n",
    "                created_utc,\n",
    "                regexp_extract(comment, '/u/([^\\\\s/]+)', 1).alias('user_reference'),\n",
    "                regexp_extract(comment, '/r/([^\\\\s/]+)', 1).alias('post_reference'),\n",
    "                regexp_extract(comment, 'http[s]?://([^\\\\s/]+)', 1).alias('site_reference')\n",
    "            FROM raw\n",
    "        \"\"\")\n",
    "         # Save processed data to a temporary table\n",
    "        references_df.createOrReplaceTempView(\"metrics\")\n",
    "        references_df.write.json(output_path)\n",
    "        references_df.write.csv(output_path + \".csv\", header=True)\n",
    "\n",
    "        # Preprocess comments\n",
    "        preprocessed_comments = references_df.select(\"comment\").rdd.flatMap(lambda x: x).map(preprocess_comment)\n",
    "\n",
    "        # Calculate TF-IDF\n",
    "        vectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"raw_features\")\n",
    "        vectorizer_model = vectorizer.fit(preprocessed_comments)\n",
    "        count_vectorized = vectorizer_model.transform(preprocessed_comments)\n",
    "\n",
    "        idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "        idf_model = idf.fit(count_vectorized)\n",
    "        tfidf = idf_model.transform(count_vectorized)\n",
    "\n",
    "        # Get top 10 important words\n",
    "        vocab = vectorizer_model.vocabulary\n",
    "        top_10_words = idf_model.idf.toArray().argsort()[-10:]\n",
    "        top_10_words = [vocab[idx] for idx in top_10_words]\n",
    "        print(\"Top 10 important words:\")\n",
    "        print(top_10_words)\n",
    "\n",
    "comments.foreachRDD(process_rdd)\n",
    "\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T17:09:19.726990Z",
     "start_time": "2023-06-20T17:09:19.605012Z"
    }
   },
   "source": [
    "## Preprocess and Save Transformed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-06-21T13:21:19.299536Z",
     "start_time": "2023-06-21T13:21:17.992026Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/21 15:21:19 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<pyspark.sql.streaming.query.StreamingQuery at 0x10e294430>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col, from_unixtime\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "host = \"localhost\"\n",
    "port = 9999\n",
    "\n",
    "# Create a SparkSession and StreamingContext\n",
    "spark_conf = SparkConf().setAppName(\"reddit\")\n",
    "ss1 = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "\n",
    "# ---- UPDATE BELOW ACCORDING TO THE LOCATION IN THE FIRST CELL -----\n",
    "input_path = \"./data/raw/reddit_v5/*/*.json\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"comment\", StringType(), True),\n",
    "    StructField(\"parent_comment\", StringType(), True),\n",
    "    StructField(\"post\", StringType(), True),\n",
    "    StructField(\"created_utc\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), True)\n",
    "])\n",
    "\n",
    "streaming_df = ss1.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"path\", input_path) \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "transformed_df = streaming_df \\\n",
    "    .withColumn('created_utc2', F.col('created_utc').cast(\"float\")) \\\n",
    "    .withColumn('created_utc3', F.col('created_utc2').cast(\"int\")) \\\n",
    "    .withColumn('created_utc_ts', F.from_unixtime(F.col('created_utc3')).cast(TimestampType())) \\\n",
    "    .withWatermark(\"created_utc_ts\", \"5 seconds\") \\\n",
    "    .groupBy(F.col(\"author\"), F.window(F.col(\"created_utc_ts\"), windowDuration=\"60 seconds\")) \\\n",
    "    .agg({\"created_utc_ts\":'max', \"comment\":'count'})\n",
    "\n",
    "# Save output to disk\n",
    "# ---- UPDATE BELOW EACH TIME YOU RERUN THIS CELL -----\n",
    "output_path = \"./data/processed/reddit_v1\"\n",
    "checkpt_path = \"./metadata/processed/reddit_v1\"\n",
    "\n",
    "transformed_df.writeStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"checkpointLocation\", checkpt_path) \\\n",
    "    .option(\"path\", output_path) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2023-06-21T07:25:25.736805Z",
     "start_time": "2023-06-21T07:25:25.698128Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/21 09:25:25 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/06/21 09:25:25 WARN StreamingQueryManager: Stopping existing streaming query [id=3fb4c12d-0721-4f23-b23e-10e78a952527, runId=d17f5d47-0a56-4317-9aca-d23cb06bc3e5], as a new run is being started.\n"
     ]
    }
   ],
   "source": [
    "query = transformed_df.writeStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"checkpointLocation\", checkpt_path) \\\n",
    "    .option(\"path\", output_path) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "# Stop the streaming query\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "1db35aecd77fcc020a8642668a5b7619c380276ef0ce04f324d75e02f2d7512b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
