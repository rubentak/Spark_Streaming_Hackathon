{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Consumer - Structured Streaming\n",
    "\n",
    "This notebook shows \n",
    " * how to receive the reddit json content coming through a socket, and save this raw data locally to disk (using spark dstreams).\n",
    " * how to use spark structured streaming to load data from disk and process it.\n",
    " * how to converts dstream to spark dataframe, \n",
    " * how to make dstreams and structured streaming dataframes available in SQL interfaces.\n",
    " * how to join streaming data with static data.\n",
    "\n",
    "The process:\n",
    " * Execute the first cell to save the streaming data to disk. The code will keep running undefinitely until the notebook kernel is terminated. If you update the output format (new fields, etc.), you need to increment the `base_path` variables before rerunning the cell, to have a consistent dataset.\n",
    " * Execute the second cell to run the processing on the data saved to disk. The code will run on all previously saved files and then will be triggered each time a new file is added (from the code in the first cell). The code will keep running undefinitely until you rerun that cell. This allows updating the cell code and trying it, while always running on all previous files. Before each retry, you need to increment `output_path` and `checkpt_path` variables.\n",
    " * The 3rd cell is provided for testing purposes.\n",
    "\n",
    "This notebook is meant to work in parallel to the content producer (`reddit_producer.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-06-20T15:19:28.197425Z",
     "start_time": "2023-06-20T15:19:21.566863Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/20 17:19:24 WARN Utils: Your hostname, MacBook-Pro-van-Ruben-2.local resolves to a loopback address: 127.0.0.1; using 192.168.0.101 instead (on interface en0)\n",
      "23/06/20 17:19:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/20 17:19:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/20 17:19:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/06/20 17:19:26 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "/Users/erictak/miniconda3/lib/python3.10/site-packages/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# This cell shows \n",
    "#  * how to receive the reddit json content coming through a socket, and save this raw data locally to disk (using spark dstreams).\n",
    "#  * how to converts dstream to spark dataframe, \n",
    "#  * how to make dstreams available in SQL interfaces.\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Create a SparkSession and StreamingContext\n",
    "spark_conf = SparkConf().setAppName(\"reddit\")\n",
    "ss1 = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "ssc = StreamingContext(ss1.sparkContext, 5)\n",
    "\n",
    "# Create a DStream\n",
    "lines = ssc.socketTextStream(\"localhost\", 9998)\n",
    "comments = lines.map(lambda json_data: json.loads(json_data))\n",
    "comments.pprint()\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"comment\",StringType(),True),\n",
    "    StructField(\"prev_comment\",StringType(),True),\n",
    "    StructField(\"post\",StringType(),True),\n",
    "    StructField(\"author\",StringType(),True),\n",
    "    StructField(\"created_utc\",StringType(),True),\n",
    "    ])\n",
    "\n",
    "# ---- UPDATE BELOW IF YOU RERUN THIS CELL WITH A NEW OUTPUT FORMAT -----\n",
    "base_path = \"./data/raw/reddit_v5\"\n",
    "\n",
    "# Convert each RDD in the DStream to a DataFrame\n",
    "def process_rdd(time, rdd):\n",
    "    if not rdd.isEmpty():\n",
    "        df = ss1.createDataFrame(rdd, schema)\n",
    "        df.registerTempTable(\"comments\")\n",
    "        df.persist()\n",
    "        output_path = f\"{base_path}/{time.strftime('%Y%m%d%H%M%S')}\"\n",
    "        df.write.json(output_path)\n",
    "        df.show()\n",
    "\n",
    "comments.foreachRDD(process_rdd)\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()  \n",
    "# no ssc.awaitTermination() added here to make the cell non blocking and to use other cell in parallel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This notebook shows \n",
    "#  * how to use spark structured streaming to load data from disk and process it.\n",
    "#  * how to make structured streaming dataframes available in SQL interfaces.\n",
    "#  * how to join streaming data with static datasets.\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType, IntegerType, LongType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "host = \"localhost\"\n",
    "port = 9998\n",
    "\n",
    "# Create a SparkSession and StreamingContext\n",
    "spark_conf = SparkConf().setAppName(\"reddit\")\n",
    "ss1 = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "\n",
    "# ---- UPDATE BELOW ACCORDING TO THE LOCATION IN THE FIRST CELL -----\n",
    "input_path = \"./data/raw/reddit_v5/*/*.json\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"comment\",StringType(),True),\n",
    "    StructField(\"prev_comment\",StringType(),True),\n",
    "    StructField(\"post\",StringType(),True),\n",
    "    StructField(\"author\",StringType(),True),\n",
    "    StructField(\"created_utc\",StringType(),True),\n",
    "    ])\n",
    "\n",
    "streaming_df = ss1.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"path\", input_path) \\\n",
    "    .load()\n",
    "\n",
    "transformed_df = streaming_df \\\n",
    "    .withColumn('created_utc2', F.col('created_utc').cast(\"float\")) \\\n",
    "    .withColumn('created_utc3', F.col('created_utc2').cast(\"int\")) \\\n",
    "    .withColumn('created_utc_ts', F.from_unixtime(F.col('created_utc3')).cast(TimestampType())) \\\n",
    "    .withWatermark(\"created_utc_ts\", \"5 seconds\") \\\n",
    "    .groupBy(F.col(\"author\"), F.window(F.col(\"created_utc_ts\"), windowDuration=\"60 seconds\")) \\\n",
    "    .agg({\"created_utc_ts\":'max', \"comment\":'count'})\n",
    "\n",
    "\n",
    "#transformed_df.createOrReplaceTempView(\"stream\")\n",
    "#transformed_df = ss1.sql(\"\"\"\n",
    "#    SELECT *\n",
    "#    FROM stream as ct\n",
    "#    \"\"\")\n",
    "# streaming df can be joined with static tables.\n",
    "\n",
    "\n",
    "# Save output to disk\n",
    "# ---- UPDATE BELOW EACH TIME YOU RERUN THIS CELL -----\n",
    "output_path = \"./data/processed/reddit_v1\"\n",
    "checkpt_path = \"./metadata/processed/reddit_v1\"\n",
    "\n",
    "transformed_df.writeStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"checkpointLocation\", checkpt_path) \\\n",
    "    .option(\"path\", output_path) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "# For testing, output to console\n",
    "#query = transformed_df.writeStream \\\n",
    "#    .format(\"console\") \\\n",
    "#    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To mix static table (with histoical content)\n",
    "# and dynamic table (with content from current window)\n",
    "\n",
    "# Loading static table\n",
    "base_path = \"./data/raw/reddit_v5/*/*.json\"\n",
    "historical = ss1.read.json(base_path)\n",
    "historical.createOrReplaceTempView('historical')\n",
    "historical.show()\n",
    "\n",
    "# Mixing static table with histoical content and dynamic table with content from current window\n",
    "ss1.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM comments as ct\n",
    "    LEFT JOIN historical as ht on ht.author=ct.author\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ssc.stop()\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
